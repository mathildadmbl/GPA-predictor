{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Pipeline : Predicting a student's GPA from his performances\n",
    "DEMBELE Mathilda, MARSOT Elouan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Starting the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des librairies pyspark\n",
    "\n",
    "# Initialisation de Spark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import ML pyspark modules\n",
    "# some examples\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"GPAPredictor\") \\\n",
    "    .getOrCreate()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASET DESCRIPTION\n",
    "- TRAIN : 1531 samples \n",
    "- TEST : 384 samples \n",
    "- each line in the dataset stands for some student\n",
    "- each column is a feature of performance for the student\n",
    "\n",
    "14 features :\n",
    "- StudentID : int, a four-figures unique number \n",
    "- Age : int \n",
    "- Gender : binary, 0 for a man, 1 for a woman \n",
    "- Ethnicity : categorial (Caucasian, Asian, African American, Other)\n",
    "- ParentalEducation : categorial (High School, Bachelor, Some College, Higher)\n",
    "- StudyTimeWeekly : float, nb of hours per week \n",
    "- Absences : int \n",
    "- Tutoring : binary, 1 if yes, 0 otherwise \n",
    "- ParentalSupport : categorial (Low, Moderate, High, Very High)\n",
    "- Extracurricular : binary\n",
    "- Sports : binary \n",
    "- Music : binary \n",
    "- Volunteering : binary \n",
    "\n",
    "- GPA : float (from 0 to 4)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "fileNameTrain = \"datasets/train.csv\"\n",
    "fileNameTest = \"datasets/test.csv\"\n",
    "\n",
    "# Reading the datasets\n",
    "train_set = spark.read.csv(fileNameTrain, header=True, inferSchema=True)\n",
    "test_set = spark.read.csv(fileNameTest, header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StudentID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- ParentalEducation: string (nullable = true)\n",
      " |-- StudyTimeWeekly: double (nullable = true)\n",
      " |-- Absences: integer (nullable = true)\n",
      " |-- Tutoring: integer (nullable = true)\n",
      " |-- ParentalSupport: string (nullable = true)\n",
      " |-- Extracurricular: integer (nullable = true)\n",
      " |-- Sports: integer (nullable = true)\n",
      " |-- Music: integer (nullable = true)\n",
      " |-- Volunteering: integer (nullable = true)\n",
      " |-- GPA: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- StudentID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- ParentalEducation: string (nullable = true)\n",
      " |-- StudyTimeWeekly: double (nullable = true)\n",
      " |-- Absences: integer (nullable = true)\n",
      " |-- Tutoring: integer (nullable = true)\n",
      " |-- ParentalSupport: string (nullable = true)\n",
      " |-- Extracurricular: integer (nullable = true)\n",
      " |-- Sports: integer (nullable = true)\n",
      " |-- Music: integer (nullable = true)\n",
      " |-- Volunteering: integer (nullable = true)\n",
      " |-- GPA: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(StudentID=2340, Age=16, Gender=1, Ethnicity='Other', ParentalEducation='Higher', StudyTimeWeekly=5.04404804318662, Absences=25, Tutoring=1, ParentalSupport='Moderate', Extracurricular=1, Sports=0, Music=0, Volunteering=0, GPA=0.886889415770466)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECKING THAT THEY HAVE THE SAME SCHEMA\n",
    "train_set.printSchema()\n",
    "test_set.printSchema()\n",
    "train_set.take(1)\n",
    "test_set.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-----------+--------------+----------------------+--------------------+-------------+-------------+--------------------+--------------------+-----------+----------+-----------------+--------+\n",
      "|sum(StudentID)|sum(Age)|sum(Gender)|sum(Ethnicity)|sum(ParentalEducation)|sum(StudyTimeWeekly)|sum(Absences)|sum(Tutoring)|sum(ParentalSupport)|sum(Extracurricular)|sum(Sports)|sum(Music)|sum(Volunteering)|sum(GPA)|\n",
      "+--------------+--------+-----------+--------------+----------------------+--------------------+-------------+-------------+--------------------+--------------------+-----------+----------+-----------------+--------+\n",
      "|             0|       0|          0|             0|                   142|                   0|            0|            0|                 132|                   0|          0|         0|                0|       0|\n",
      "+--------------+--------+-----------+--------------+----------------------+--------------------+-------------+-------------+--------------------+--------------------+-----------+----------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "train_set.select([(col(c).isNull().cast(\"int\")).alias(c) for c in train_set.columns]).groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only ParentalEducation and ParentalSupport have some missing values : this should be handled in our future pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High School\n",
      "Higher\n",
      "Bachelor\n",
      "Some College\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "categories = train_set.select(\"ParentalEducation\").distinct().collect()\n",
    "for row in categories:\n",
    "    print(row[\"ParentalEducation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High\n",
      "Very High\n",
      "Low\n",
      "Moderate\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "categories = train_set.select(\"ParentalSupport\").distinct().collect()\n",
    "for row in categories:\n",
    "    print(row[\"ParentalSupport\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is an order among those categories, let's map them then we will try to imput them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_mapping_education = {\"High School\": 1, \"Some College\": 2, \"Bachelor\": 3, \"Higher\": 4}\n",
    "ordinal_mapping_support = {\"Low\": 0, \"Moderate\": 1, \"High\": 2, \"Very High\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "class OrdinalEncoder(Transformer):\n",
    "    def __init__(self, mappings, inputCols, outputCols):\n",
    "        super(OrdinalEncoder, self).__init__()\n",
    "        self.mappings = mappings  \n",
    "        self.inputCols = inputCols\n",
    "        self.outputCols = outputCols\n",
    "\n",
    "    def _transform(self, df):\n",
    "        for inputCol, outputCol, mapping in zip(self.inputCols, self.outputCols, self.mappings):\n",
    "            expr = None\n",
    "            for category, value in mapping.items():\n",
    "                if expr is None:\n",
    "                    expr = when(col(inputCol) == category, value)\n",
    "                else:\n",
    "                    expr = expr.when(col(inputCol) == category, value)\n",
    "            df = df.withColumn(outputCol, expr.otherwise(None)) \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "\"\"\"\n",
    "ORDINAL ENCODING \n",
    "maps categorical values in the columns ParentalEducation and ParentalSupport to numeric values based \n",
    "on a predefined mapping\n",
    "\"\"\"\n",
    "encoder = OrdinalEncoder(\n",
    "    mappings=[ordinal_mapping_education, ordinal_mapping_support],\n",
    "    inputCols=[\"ParentalEducation\", \"ParentalSupport\"],\n",
    "    outputCols=[\"ParentalEducationEncoded\", \"ParentalSupportEncoded\"]\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IMPUTING \n",
    "replaces missing values in the same columns \n",
    "mode strategy = replace with the most frequently occurring value\n",
    "\"\"\"\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"ParentalEducationEncoded\", \"ParentalSupportEncoded\"],\n",
    "    outputCols=[\"ParentalEducationImputed\", \"ParentalSupportImputed\"]\n",
    ").setStrategy(\"mode\")\n",
    "\n",
    "\n",
    "intermediate_assembler = VectorAssembler(\n",
    "    inputCols=[\"ParentalSupportImputed\", \"ParentalEducationImputed\"],  \n",
    "    outputCol=\"parents_features_processed\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Processing other features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\"Age\", \"StudyTimeWeekly\", \"Absences\"]\n",
    "categorical_features = [\"Ethnicity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, VectorAssembler, StringIndexer\n",
    "\n",
    "\"\"\"\n",
    "ONE-HOT ENCODING\n",
    "transform the categorical feature Ethnicity into a numeric format \n",
    "and then into a sparse one-hot encoded vector\n",
    "\"\"\"\n",
    "indexer = StringIndexer(inputCol=\"Ethnicity\", outputCol=\"Ethnicity_indexed\")\n",
    "onehot_encoder= OneHotEncoder(inputCol=\"Ethnicity_indexed\", outputCol=\"Ethnicity_encoded\")\n",
    "\n",
    "\"\"\"\n",
    "VECTOR ASSEMBLY \n",
    "combines multiple numeric columns into a single vector column called numeric_features\n",
    "--> input features in vector form \n",
    "\"\"\"\n",
    "#numerical_features = [\"Age\", \"StudyTimeWeekly\", \"Absences\"]\n",
    "numeric_assembler = VectorAssembler(inputCols=numerical_features, outputCol=\"numeric_features\")\n",
    "\n",
    "\"\"\"\n",
    "SCALING \n",
    "scales the numeric_features vector to have zero mean and unit variance\n",
    "\"\"\"\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\")\n",
    "\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"scaled_numeric_features\", \"Ethnicity_encoded\", \"parents_features_processed\", \"Gender\", \"Tutoring\", \"Extracurricular\", \"Sports\", \"Music\", \"Volunteering\"],  # Scaled and encoded features\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. ML model and chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\"\"\"\n",
    "TO PREDICT THE GPA : \n",
    "- Linear Regression : for interpretability and simplicity\n",
    "- RANDOM FOREST REGRESSOR or GRADIENT-BOOSTED TREE REGRESSOR: for data with complex, non-linear patterns \"\"\"\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#lr = LinearRegression(featuresCol=\"features\", labelCol=\"GPA\")\n",
    "#rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"GPA\")\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"GPA\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    encoder, # encode string values \n",
    "    imputer,    # handle missing values \n",
    "    intermediate_assembler, # assemble parents features into a vector\n",
    "    indexer,    # index categorical features \n",
    "    onehot_encoder, # one-hot encode \n",
    "    numeric_assembler,  # assemble numerical features \n",
    "    scaler,     # scale numerical features\n",
    "    final_assembler,    # assemble all features \n",
    "    gbt # ML algorithm\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after encoder: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded']\n",
      "Columns after imputer: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed']\n",
      "Columns after intermediate assembler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed']\n",
      "+--------------------------+\n",
      "|parents_features_processed|\n",
      "+--------------------------+\n",
      "|[1.0,4.0]                 |\n",
      "|[1.0,3.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[1.0,4.0]                 |\n",
      "|[2.0,2.0]                 |\n",
      "|[2.0,1.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[4.0,2.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[0.0,1.0]                 |\n",
      "|[0.0,2.0]                 |\n",
      "|[0.0,1.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[2.0,2.0]                 |\n",
      "+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "After StringIndexer: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed']\n",
      "After OneHotEncoder: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded']\n",
      "After VectorAssembler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded', 'numeric_features']\n",
      "After StandardScaler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded', 'numeric_features', 'scaled_numeric_features']\n",
      "After Final Assembler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded', 'numeric_features', 'scaled_numeric_features', 'features']\n"
     ]
    }
   ],
   "source": [
    "# TESTING EACH ELEMENT OF THE PIPELINE \n",
    "\n",
    "# Step 1: Apply the encoder\n",
    "encoded_df = encoder.transform(test_set)\n",
    "print(\"Columns after encoder:\", encoded_df.columns)\n",
    "\n",
    "# Step 2: Apply the imputer\n",
    "imputed_df = imputer.fit(encoded_df).transform(encoded_df)\n",
    "print(\"Columns after imputer:\", imputed_df.columns)\n",
    "\n",
    "# Step 3: Apply the intermediate assembler\n",
    "assembled_df = intermediate_assembler.transform(imputed_df)\n",
    "print(\"Columns after intermediate assembler:\", assembled_df.columns)\n",
    "assembled_df.select(\"parents_features_processed\").show(truncate=False)\n",
    "\n",
    "indexed_df = indexer.fit(assembled_df).transform(assembled_df)\n",
    "print(\"After StringIndexer:\", indexed_df.columns)\n",
    "\n",
    "# Step 2: Apply OneHotEncoder\n",
    "encoded_df = onehot_encoder.fit(indexed_df).transform(indexed_df)\n",
    "print(\"After OneHotEncoder:\", encoded_df.columns)\n",
    "\n",
    "# Step 3: Apply VectorAssembler\n",
    "assembled_df = numeric_assembler.transform(encoded_df)\n",
    "print(\"After VectorAssembler:\", assembled_df.columns)\n",
    "\n",
    "# Step 4: Apply StandardScaler\n",
    "scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "print(\"After StandardScaler:\", scaled_df.columns)\n",
    "\n",
    "# Step 5: Apply Final Assembler\n",
    "final_df = final_assembler.transform(scaled_df)\n",
    "print(\"After Final Assembler:\", final_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Fitting the pipeline on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = pipeline.fit(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|GPA              |prediction         |\n",
      "+-----------------+-------------------+\n",
      "|0.886889415770466|0.9727391403691974 |\n",
      "|2.23469628732449 |2.335452454402756  |\n",
      "|0.875367123899009|0.7666055944440374 |\n",
      "|0.648705394877608|0.48681221942689495|\n",
      "|3.46368754912343 |3.3225752150522894 |\n",
      "|3.10949357887921 |3.1795632390698363 |\n",
      "|3.0092383227043  |2.462957335582419  |\n",
      "|2.67682739428337 |2.487941445459509  |\n",
      "|2.94871767191192 |3.0405219610634155 |\n",
      "|1.70361180323237 |1.6085162975087512 |\n",
      "|0.655954077962677|0.8389381365573322 |\n",
      "|1.97279141390272 |1.4271693337155378 |\n",
      "|2.23217527771598 |1.9639529561985694 |\n",
      "|2.97440601491104 |2.4883209962013133 |\n",
      "|1.60941027714416 |1.7198115303771646 |\n",
      "|2.30759662333527 |2.673305545430982  |\n",
      "|1.78996687416233 |1.7377355494889484 |\n",
      "|2.36601087218728 |2.4604526012399575 |\n",
      "|0.864785083851288|1.2048523040810628 |\n",
      "|2.85480392898132 |2.716027484743984  |\n",
      "+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_test_set = model.transform(test_set)\n",
    "\n",
    "# PREDICTIONS\n",
    "transformed_test_set.select(\"GPA\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"GPA\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"GPA\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"GPA\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = evaluator_rmse.evaluate(transformed_test_set)\n",
    "mae = evaluator_mae.evaluate(transformed_test_set)\n",
    "r2 = evaluator_r2.evaluate(transformed_test_set)\n",
    "\n",
    "# Print results\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R² (Coefficient of Determination): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on a random individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Select a random individual from the test set\n",
    "# WARNING : DON'T REEXECUTE THE CELL TO COMPARE THE ALGORITHMS ON THE SAME INDIVIDUAL \n",
    "random_individual = test_set.limit(27) \n",
    "print(random_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Apply the pipeline to the random individual\n",
    "prediction = model.transform(random_individual)\n",
    "\n",
    "# Step 3: Display the result\n",
    "prediction.select(\"GPA\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross val for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: [0.27689339193059037]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"GPA\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=[{}],\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cvModel = crossval.fit(train_set)\n",
    "\n",
    "print(\"RMSE:\", cvModel.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
