{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Pipeline : Predicting a student's GPA from his performances\n",
    "DEMBELE Mathilda, MARSOT Elouan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Starting the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des librairies pyspark\n",
    "\n",
    "# Initialisation de Spark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import ML pyspark modules\n",
    "# some examples\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"GPAPredictor\") \\\n",
    "    .getOrCreate()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASET DESCRIPTION\n",
    "- TRAIN : 1531 samples \n",
    "- TEST : 384 samples \n",
    "- each line in the dataset stands for some student\n",
    "- each column is a feature of performance for the student\n",
    "\n",
    "14 features :\n",
    "- StudentID : int, a four-figures unique number \n",
    "- Age : int \n",
    "- Gender : binary, 0 for a man, 1 for a woman \n",
    "- Ethnicity : categorial (Caucasian, Asian, African American, Other)\n",
    "- ParentalEducation : categorial (High School, Bachelor, Some College, Higher)\n",
    "- StudyTimeWeekly : float, nb of hours per week \n",
    "- Absences : int \n",
    "- Tutoring : binary, 1 if yes, 0 otherwise \n",
    "- ParentalSupport : categorial (Low, Moderate, High, Very High)\n",
    "- Extracurricular : binary\n",
    "- Sports : binary \n",
    "- Music : binary \n",
    "- Volunteering : binary \n",
    "\n",
    "- GPA : float (from 0 to 4)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "fileNameTrain = \"datasets/train.csv\"\n",
    "fileNameTest = \"datasets/test.csv\"\n",
    "\n",
    "# Reading the datasets\n",
    "train_set = spark.read.csv(fileNameTrain, header=True, inferSchema=True)\n",
    "test_set = spark.read.csv(fileNameTest, header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StudentID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- ParentalEducation: string (nullable = true)\n",
      " |-- StudyTimeWeekly: double (nullable = true)\n",
      " |-- Absences: integer (nullable = true)\n",
      " |-- Tutoring: integer (nullable = true)\n",
      " |-- ParentalSupport: string (nullable = true)\n",
      " |-- Extracurricular: integer (nullable = true)\n",
      " |-- Sports: integer (nullable = true)\n",
      " |-- Music: integer (nullable = true)\n",
      " |-- Volunteering: integer (nullable = true)\n",
      " |-- GPA: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- StudentID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- ParentalEducation: string (nullable = true)\n",
      " |-- StudyTimeWeekly: double (nullable = true)\n",
      " |-- Absences: integer (nullable = true)\n",
      " |-- Tutoring: integer (nullable = true)\n",
      " |-- ParentalSupport: string (nullable = true)\n",
      " |-- Extracurricular: integer (nullable = true)\n",
      " |-- Sports: integer (nullable = true)\n",
      " |-- Music: integer (nullable = true)\n",
      " |-- Volunteering: integer (nullable = true)\n",
      " |-- GPA: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(StudentID=2340, Age=16, Gender=1, Ethnicity='Other', ParentalEducation='Higher', StudyTimeWeekly=5.04404804318662, Absences=25, Tutoring=1, ParentalSupport='Moderate', Extracurricular=1, Sports=0, Music=0, Volunteering=0, GPA=0.886889415770466)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECKING THAT THEY HAVE THE SAME SCHEMA\n",
    "train_set.printSchema()\n",
    "test_set.printSchema()\n",
    "train_set.take(1)\n",
    "test_set.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-----------+--------------+----------------------+--------------------+-------------+-------------+--------------------+--------------------+-----------+----------+-----------------+--------+\n",
      "|sum(StudentID)|sum(Age)|sum(Gender)|sum(Ethnicity)|sum(ParentalEducation)|sum(StudyTimeWeekly)|sum(Absences)|sum(Tutoring)|sum(ParentalSupport)|sum(Extracurricular)|sum(Sports)|sum(Music)|sum(Volunteering)|sum(GPA)|\n",
      "+--------------+--------+-----------+--------------+----------------------+--------------------+-------------+-------------+--------------------+--------------------+-----------+----------+-----------------+--------+\n",
      "|             0|       0|          0|             0|                   142|                   0|            0|            0|                 132|                   0|          0|         0|                0|       0|\n",
      "+--------------+--------+-----------+--------------+----------------------+--------------------+-------------+-------------+--------------------+--------------------+-----------+----------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "train_set.select([(col(c).isNull().cast(\"int\")).alias(c) for c in train_set.columns]).groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only ParentalEducation and ParentalSupport have some missing values : this should be handled in our future pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High School\n",
      "Higher\n",
      "Bachelor\n",
      "Some College\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "categories = train_set.select(\"ParentalEducation\").distinct().collect()\n",
    "for row in categories:\n",
    "    print(row[\"ParentalEducation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High\n",
      "Very High\n",
      "Low\n",
      "Moderate\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "categories = train_set.select(\"ParentalSupport\").distinct().collect()\n",
    "for row in categories:\n",
    "    print(row[\"ParentalSupport\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is an order among those categories, let's map them then we will try to imput them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_mapping_education = {\"High School\": 1, \"Some College\": 2, \"Bachelor\": 3, \"Higher\": 4}\n",
    "ordinal_mapping_support = {\"Low\": 0, \"Moderate\": 1, \"High\": 2, \"Very High\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "class OrdinalEncoder(Transformer):\n",
    "    def __init__(self, mappings, inputCols, outputCols):\n",
    "        super(OrdinalEncoder, self).__init__()\n",
    "        self.mappings = mappings  \n",
    "        self.inputCols = inputCols\n",
    "        self.outputCols = outputCols\n",
    "\n",
    "    def _transform(self, df):\n",
    "        for inputCol, outputCol, mapping in zip(self.inputCols, self.outputCols, self.mappings):\n",
    "            expr = None\n",
    "            for category, value in mapping.items():\n",
    "                if expr is None:\n",
    "                    expr = when(col(inputCol) == category, value)\n",
    "                else:\n",
    "                    expr = expr.when(col(inputCol) == category, value)\n",
    "            df = df.withColumn(outputCol, expr.otherwise(None)) \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "\"\"\"\n",
    "ORDINAL ENCODING \n",
    "maps categorical values in the columns ParentalEducation and ParentalSupport to numeric values based \n",
    "on a predefined mapping\n",
    "\"\"\"\n",
    "encoder = OrdinalEncoder(\n",
    "    mappings=[ordinal_mapping_education, ordinal_mapping_support],\n",
    "    inputCols=[\"ParentalEducation\", \"ParentalSupport\"],\n",
    "    outputCols=[\"ParentalEducationEncoded\", \"ParentalSupportEncoded\"]\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IMPUTING \n",
    "replaces missing values in the same columns \n",
    "mode strategy = replace with the most frequently occurring value\n",
    "\"\"\"\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"ParentalEducationEncoded\", \"ParentalSupportEncoded\"],\n",
    "    outputCols=[\"ParentalEducationImputed\", \"ParentalSupportImputed\"]\n",
    ").setStrategy(\"mode\")\n",
    "\n",
    "\n",
    "intermediate_assembler = VectorAssembler(\n",
    "    inputCols=[\"ParentalSupportImputed\", \"ParentalEducationImputed\"],  \n",
    "    outputCol=\"parents_features_processed\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Processing other features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\"Age\", \"StudyTimeWeekly\", \"Absences\"]\n",
    "categorical_features = [\"Ethnicity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, VectorAssembler, StringIndexer\n",
    "\n",
    "\"\"\"\n",
    "ONE-HOT ENCODING\n",
    "transform the categorical feature Ethnicity into a numeric format \n",
    "and then into a sparse one-hot encoded vector\n",
    "\"\"\"\n",
    "indexer = StringIndexer(inputCol=\"Ethnicity\", outputCol=\"Ethnicity_indexed\")\n",
    "onehot_encoder= OneHotEncoder(inputCol=\"Ethnicity_indexed\", outputCol=\"Ethnicity_encoded\")\n",
    "\n",
    "\"\"\"\n",
    "VECTOR ASSEMBLY \n",
    "combines multiple numeric columns into a single vector column called numeric_features\n",
    "--> input features in vector form \n",
    "\"\"\"\n",
    "#numerical_features = [\"Age\", \"StudyTimeWeekly\", \"Absences\"]\n",
    "numeric_assembler = VectorAssembler(inputCols=numerical_features, outputCol=\"numeric_features\")\n",
    "\n",
    "\"\"\"\n",
    "SCALING \n",
    "scales the numeric_features vector to have zero mean and unit variance\n",
    "\"\"\"\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\")\n",
    "\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"scaled_numeric_features\", \"Ethnicity_encoded\", \"parents_features_processed\", \"Gender\", \"Tutoring\", \"Extracurricular\", \"Sports\", \"Music\", \"Volunteering\"],  # Scaled and encoded features\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. ML model and chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\"\"\"\n",
    "TO PREDICT THE GPA : \n",
    "- Linear Regression : for interpretability and simplicity\n",
    "- RANDOM FOREST REGRESSOR or GRADIENT-BOOSTED TREE REGRESSOR: for data with complex, non-linear patterns \"\"\"\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#lr = LinearRegression(featuresCol=\"features\", labelCol=\"GPA\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"GPA\")\n",
    "#gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"GPA\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    encoder, # encode string values \n",
    "    imputer,    # handle missing values \n",
    "    intermediate_assembler, # assemble parents features into a vector\n",
    "    indexer,    # index categorical features \n",
    "    onehot_encoder, # one-hot encode \n",
    "    numeric_assembler,  # assemble numerical features \n",
    "    scaler,     # scale numerical features\n",
    "    final_assembler,    # assemble all features \n",
    "    rf # ML algorithm\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after encoder: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded']\n",
      "Columns after imputer: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed']\n",
      "Columns after intermediate assembler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed']\n",
      "+--------------------------+\n",
      "|parents_features_processed|\n",
      "+--------------------------+\n",
      "|[1.0,4.0]                 |\n",
      "|[1.0,3.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[1.0,4.0]                 |\n",
      "|[2.0,2.0]                 |\n",
      "|[2.0,1.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[4.0,2.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[0.0,1.0]                 |\n",
      "|[0.0,2.0]                 |\n",
      "|[0.0,1.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,1.0]                 |\n",
      "|[1.0,2.0]                 |\n",
      "|[2.0,2.0]                 |\n",
      "+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "After StringIndexer: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed']\n",
      "After OneHotEncoder: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded']\n",
      "After VectorAssembler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded', 'numeric_features']\n",
      "After StandardScaler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded', 'numeric_features', 'scaled_numeric_features']\n",
      "After Final Assembler: ['StudentID', 'Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GPA', 'ParentalEducationEncoded', 'ParentalSupportEncoded', 'ParentalEducationImputed', 'ParentalSupportImputed', 'parents_features_processed', 'Ethnicity_indexed', 'Ethnicity_encoded', 'numeric_features', 'scaled_numeric_features', 'features']\n"
     ]
    }
   ],
   "source": [
    "# TESTING EACH ELEMENT OF THE PIPELINE \n",
    "\n",
    "# Step 1: Apply the encoder\n",
    "encoded_df = encoder.transform(test_set)\n",
    "print(\"Columns after encoder:\", encoded_df.columns)\n",
    "\n",
    "# Step 2: Apply the imputer\n",
    "imputed_df = imputer.fit(encoded_df).transform(encoded_df)\n",
    "print(\"Columns after imputer:\", imputed_df.columns)\n",
    "\n",
    "# Step 3: Apply the intermediate assembler\n",
    "assembled_df = intermediate_assembler.transform(imputed_df)\n",
    "print(\"Columns after intermediate assembler:\", assembled_df.columns)\n",
    "assembled_df.select(\"parents_features_processed\").show(truncate=False)\n",
    "\n",
    "indexed_df = indexer.fit(assembled_df).transform(assembled_df)\n",
    "print(\"After StringIndexer:\", indexed_df.columns)\n",
    "\n",
    "# Step 2: Apply OneHotEncoder\n",
    "encoded_df = onehot_encoder.fit(indexed_df).transform(indexed_df)\n",
    "print(\"After OneHotEncoder:\", encoded_df.columns)\n",
    "\n",
    "# Step 3: Apply VectorAssembler\n",
    "assembled_df = numeric_assembler.transform(encoded_df)\n",
    "print(\"After VectorAssembler:\", assembled_df.columns)\n",
    "\n",
    "# Step 4: Apply StandardScaler\n",
    "scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "print(\"After StandardScaler:\", scaled_df.columns)\n",
    "\n",
    "# Step 5: Apply Final Assembler\n",
    "final_df = final_assembler.transform(scaled_df)\n",
    "print(\"After Final Assembler:\", final_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Fitting the pipeline on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = pipeline.fit(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|GPA              |prediction        |\n",
      "+-----------------+------------------+\n",
      "|0.886889415770466|1.0734693716729076|\n",
      "|2.23469628732449 |2.375856485003866 |\n",
      "|0.875367123899009|0.9555200495897449|\n",
      "|0.648705394877608|1.1263543992012888|\n",
      "|3.46368754912343 |2.8573858091233006|\n",
      "|3.10949357887921 |2.898059266301177 |\n",
      "|3.0092383227043  |2.5227629935016895|\n",
      "|2.67682739428337 |2.4181646301863546|\n",
      "|2.94871767191192 |2.697185376698655 |\n",
      "|1.70361180323237 |1.8426397214387613|\n",
      "|0.655954077962677|0.9307377123303997|\n",
      "|1.97279141390272 |1.5031450342103088|\n",
      "|2.23217527771598 |2.112586231286649 |\n",
      "|2.97440601491104 |2.754684724392665 |\n",
      "|1.60941027714416 |1.4928273744398255|\n",
      "|2.30759662333527 |2.5102371290153886|\n",
      "|1.78996687416233 |1.5506668498742742|\n",
      "|2.36601087218728 |2.355530628298529 |\n",
      "|0.864785083851288|1.1904427981637422|\n",
      "|2.85480392898132 |2.5005745347144637|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_test_set = model.transform(test_set)\n",
    "\n",
    "# PREDICTIONS\n",
    "transformed_test_set.select(\"GPA\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.3463988158134055\n",
      "Mean Absolute Error (MAE): 0.2754870526434822\n",
      "R² (Coefficient of Determination): 0.8555148070836854\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"GPA\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"GPA\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"GPA\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = evaluator_rmse.evaluate(transformed_test_set)\n",
    "mae = evaluator_mae.evaluate(transformed_test_set)\n",
    "r2 = evaluator_r2.evaluate(transformed_test_set)\n",
    "\n",
    "# Print results\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R² (Coefficient of Determination): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on a random individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[StudentID: int, Age: int, Gender: int, Ethnicity: string, ParentalEducation: string, StudyTimeWeekly: double, Absences: int, Tutoring: int, ParentalSupport: string, Extracurricular: int, Sports: int, Music: int, Volunteering: int, GPA: double]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Select a random individual from the test set\n",
    "# WARNING : DON'T REEXECUTE THE CELL TO COMPARE THE ALGORITHMS ON THE SAME INDIVIDUAL \n",
    "random_individual = test_set.limit(27) \n",
    "print(random_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o13267.transform.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.transform(RandomForestRegressor.scala:238)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[349], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 2: Apply the pipeline to the random individual\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_individual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 3: Display the result\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prediction\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:304\u001b[0m, in \u001b[0;36mPipelineModel._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages:\n\u001b[1;32m--> 304\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o13267.transform.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\r\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.transform(RandomForestRegressor.scala:238)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: Apply the pipeline to the random individual\n",
    "prediction = model.transform(random_individual)\n",
    "\n",
    "# Step 3: Display the result\n",
    "prediction.select(\"GPA\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
